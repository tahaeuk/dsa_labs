{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here compute the TF-IDF on a corpus of newspaper headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data into the file *headlines.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the dataset\n",
    "df = pd.read_csv('headlines.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, check the dataset basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publish_date                                      headline_text\n",
      "0      20170721  algorithms can make decisions on behalf of fed...\n",
      "1      20170721  andrew forrests fmg to appeal pilbara native t...\n",
      "2      20170721                           a rural mural in thallan\n",
      "3      20170721  australia church risks becoming haven for abusers\n",
      "4      20170721  australian company usgfx embroiled in shanghai...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1999 entries, 0 to 1998\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   publish_date   1999 non-null   int64 \n",
      " 1   headline_text  1999 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 31.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TODO: Have a look at the data\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform preprocessing on this text data: tokenization, punctuation and stop words removal and stemming.\n",
    "\n",
    "Hint: to do so, use NLTK, *pandas*'s method *apply*, lambda functions and list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         [algorithm, make, decis, behalf, feder, minist]\n",
      "1       [andrew, forrest, fmg, appeal, pilbara, nativ,...\n",
      "2                                 [rural, mural, thallan]\n",
      "3                  [australia, church, risk, becom, abus]\n",
      "4       [australian, compani, usgfx, embroil, shanghai...\n",
      "                              ...                        \n",
      "1994    [constitut, avenu, win, top, prize, act, archi...\n",
      "1995                         [dark, mofo, number, crunch]\n",
      "1996    [david, petraeu, say, australia, must, firm, s...\n",
      "1997    [driverless, car, australia, face, challeng, r...\n",
      "1998               [drug, compani, criticis, price, hike]\n",
      "Name: stemmed, Length: 1999, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "df['tokenized_text'] = df['headline_text'].apply(word_tokenize)\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda x: [word for word in x if word.isalpha()])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "stemmer = PorterStemmer()\n",
    "df['stemmed'] = df['tokenized_text'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "print(df['stemmed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute now the Bag of Words for our data, using scikit-learn.\n",
    "\n",
    "Warning: since we used our own preprocessing, you have to bypass analyzer with identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999, 4165)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=identity_tokenizer)\n",
    "bow = vectorizer.fit_transform(df['stemmed'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(bow.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the shape of the BOW, the expected value is `(1999, 4165)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the Term Frequency and then the Inverse Document Frequency, and check the values are not only zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.08333333, 0.09090909, 0.1       , 0.11111111,\n",
       "       0.125     , 0.14285714, 0.16666667, 0.18181818, 0.2       ,\n",
       "       0.22222222, 0.25      , 0.28571429, 0.33333333, 0.4       ,\n",
       "       0.5       , 1.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the TF using the BOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.08333333, 0.09090909, 0.1       , 0.11111111,\n",
       "       0.125     , 0.14285714, 0.16666667, 0.18181818, 0.2       ,\n",
       "       0.22222222, 0.25      , 0.28571429, 0.33333333, 0.4       ,\n",
       "       0.5       , 1.        ])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_array = bow.toarray()\n",
    "\n",
    "total_words_in_bow = np.sum(bow_array)\n",
    "\n",
    "tf_matrix = np.zeros(bow_array.shape)\n",
    "for i in range(bow_array.shape[0]):\n",
    "    total_words_in_document = np.sum(bow_array[i])\n",
    "    if total_words_in_document != 0:\n",
    "        tf_matrix[i] = bow_array[i] / total_words_in_document\n",
    "\n",
    "unique_values = np.unique(tf_matrix.flatten())\n",
    "unique_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.28291422, 3.36629583, 3.44151925, ..., 7.60040233, 7.60040233,\n",
       "       7.60040233])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "document_frequency = np.sum(bow_array > 0, axis=0)\n",
    "idf_matrix = np.log(bow_array.shape[0] / document_frequency)\n",
    "\n",
    "sorted_indices = np.argsort(idf_matrix)\n",
    "idf_array_sorted = idf_matrix[sorted_indices]\n",
    "\n",
    "idf_array_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute finally the TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_matrix = tf_matrix.copy()\n",
    "\n",
    "for i in range(tf_matrix.shape[0]):\n",
    "    for j in range(tf_matrix.shape[1]):\n",
    "        tf_idf_matrix[i, j] = tf_matrix[i, j] * idf_matrix[j]\n",
    "\n",
    "tf_idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 10 words with the highest and lowest TF-IDF on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words with lowest TF-IDF on average:\n",
      "      Word    TF-IDF\n",
      "1648    gw  0.000317\n",
      "2515  nmfc  0.000317\n",
      "2300  melb  0.000317\n",
      "760   coll  0.000317\n",
      "1526  gcfc  0.000317\n",
      "1528  geel  0.000317\n",
      "33    adel  0.000317\n",
      "1684   haw  0.000317\n",
      "3619   syd  0.000317\n",
      "3930     v  0.000346\n",
      "\n",
      "Top 10 words with highest TF-IDF on average:\n",
      "            Word    TF-IDF\n",
      "249    australia  0.019863\n",
      "250   australian  0.019709\n",
      "2493         new  0.017262\n",
      "2245      market  0.015168\n",
      "2771       polic  0.014775\n",
      "3223         say  0.014357\n",
      "3836       trump  0.013593\n",
      "3995          wa  0.012777\n",
      "2224         man  0.012624\n",
      "3620      sydney  0.012067\n"
     ]
    }
   ],
   "source": [
    "word_tfidf_df = pd.DataFrame({'Word': feature_names, 'TF-IDF': tf_idf_matrix.mean(axis=0)})\n",
    "lowest_words = word_tfidf_df.sort_values(by='TF-IDF').head(10)\n",
    "highest_words = word_tfidf_df.sort_values(by='TF-IDF', ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 words with lowest TF-IDF on average:\")\n",
    "print(lowest_words)\n",
    "\n",
    "print(\"\\nTop 10 words with highest TF-IDF on average:\")\n",
    "print(highest_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the TF-IDF using scikit-learn on our preprocessed data (the one you used to compute the BOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2351)\t0.37940445994614136\n",
      "  (0, 1351)\t0.36465387895025764\n",
      "  (0, 354)\t0.4836746969147799\n",
      "  (0, 990)\t0.3852340690707783\n",
      "  (0, 2217)\t0.3267905927279272\n",
      "  (0, 90)\t0.4836746969147799\n",
      "  (1, 3170)\t0.327942300480771\n",
      "  (1, 3747)\t0.3229796671605963\n",
      "  (1, 2455)\t0.34651373707753075\n",
      "  (1, 2724)\t0.39063104545637256\n",
      "  (1, 159)\t0.327942300480771\n",
      "  (1, 1413)\t0.3756519462531484\n",
      "  (1, 1440)\t0.3756519462531484\n",
      "  (1, 130)\t0.35454009167689143\n",
      "  (2, 3689)\t0.6136272674992542\n",
      "  (2, 2426)\t0.6136272674992542\n",
      "  (2, 3175)\t0.4969136274673873\n",
      "  (3, 11)\t0.43643479335149765\n",
      "  (3, 345)\t0.48833892208322827\n",
      "  (3, 3116)\t0.4412675558631679\n",
      "  (3, 704)\t0.5235336535355645\n",
      "  (3, 249)\t0.3197580743141225\n",
      "  (4, 3491)\t0.4228385123497916\n",
      "  (4, 3482)\t0.3424134326053298\n",
      "  (4, 3302)\t0.4228385123497916\n",
      "  :\t:\n",
      "  (1995, 916)\t0.5485265217155414\n",
      "  (1995, 2373)\t0.5485265217155414\n",
      "  (1995, 2536)\t0.4302755758469799\n",
      "  (1995, 959)\t0.4616278141304413\n",
      "  (1996, 2701)\t0.4205359647251682\n",
      "  (1996, 972)\t0.3718074452777344\n",
      "  (1996, 2436)\t0.3718074452777344\n",
      "  (1996, 1388)\t0.3989732513653019\n",
      "  (1996, 3243)\t0.3298771294058291\n",
      "  (1996, 3430)\t0.28621722386943427\n",
      "  (1996, 3216)\t0.24055343185829647\n",
      "  (1996, 691)\t0.29548937093004335\n",
      "  (1996, 249)\t0.2270884248123804\n",
      "  (1997, 3153)\t0.4661577740693628\n",
      "  (1997, 1138)\t0.4252971334206717\n",
      "  (1997, 606)\t0.37749324835081804\n",
      "  (1997, 659)\t0.37749324835081804\n",
      "  (1997, 1310)\t0.340002064396846\n",
      "  (1997, 2846)\t0.37128230457207045\n",
      "  (1997, 249)\t0.25172409379216565\n",
      "  (1998, 903)\t0.47698111733223886\n",
      "  (1998, 1724)\t0.5118313515205288\n",
      "  (1998, 2829)\t0.38959152210216524\n",
      "  (1998, 1144)\t0.4172537091877979\n",
      "  (1998, 788)\t0.4296922961658734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "df['processed_text'] = df['stemmed'].apply(lambda x: ' '.join(x))\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(df['processed_text'])\n",
    "print(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the 10 highest and lowest TF-IDF words on average to the ones you had by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words with lowest TF-IDF on average:\n",
      "       Word  Average TF-IDF\n",
      "759    coll        0.000153\n",
      "3612    syd        0.000153\n",
      "2511   nmfc        0.000153\n",
      "2297   melb        0.000153\n",
      "1646     gw        0.000153\n",
      "33     adel        0.000153\n",
      "1524   gcfc        0.000153\n",
      "1526   geel        0.000153\n",
      "1682    haw        0.000153\n",
      "1308  fabio        0.000161\n",
      "\n",
      "Top 10 words with highest TF-IDF on average:\n",
      "            Word  Average TF-IDF\n",
      "249    australia        0.010058\n",
      "250   australian        0.009756\n",
      "2489         new        0.008802\n",
      "2766       polic        0.007834\n",
      "3216         say        0.007608\n",
      "3829       trump        0.006907\n",
      "2221         man        0.006618\n",
      "3986          wa        0.006291\n",
      "668        charg        0.006091\n",
      "3613      sydney        0.005723\n"
     ]
    }
   ],
   "source": [
    "average_tfidf_per_word = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "word_tfidf_df = pd.DataFrame({'Word': feature_names, 'Average TF-IDF': average_tfidf_per_word})\n",
    "lowest_words = word_tfidf_df.sort_values(by='Average TF-IDF').head(10)\n",
    "highest_words = word_tfidf_df.sort_values(by='Average TF-IDF', ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 words with lowest TF-IDF on average:\")\n",
    "print(lowest_words)\n",
    "\n",
    "print(\"\\nTop 10 words with highest TF-IDF on average:\")\n",
    "print(highest_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have the same words? How do you explain it?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
